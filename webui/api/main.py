#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FastAPI ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
"""

import logging
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, Any, Optional
import uvicorn

from webui.api.models import model_manager
from webui.api.inference import inference_service
from webui.utils.metrics import metrics_calculator
from webui.config.settings import settings

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=getattr(logging, settings.LOG_LEVEL),
    format=settings.LOG_FORMAT
)
logger = logging.getLogger(__name__)

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
app = FastAPI(
    title="ğŸ¤– LLMæ¯”è¼ƒAPI",
    description="ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒAPI",
    version="1.0.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc"
)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class CompareRequest(BaseModel):
    prompt: str
    max_new_tokens: Optional[int] = 256
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    top_k: Optional[int] = None

class SingleInferenceRequest(BaseModel):
    prompt: str
    model_type: str = "finetuned"  # "base" or "finetuned"
    max_new_tokens: Optional[int] = 256
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    top_k: Optional[int] = None

# APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/api/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "healthy",
        "version": "1.0.0",
        "models_loaded": model_manager.is_loaded()
    }

@app.post("/api/models/load")
async def load_models(background_tasks: BackgroundTasks):
    """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
    if model_manager.is_loaded():
        return {
            "status": "already_loaded",
            "message": "ãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«èª­ã¿è¾¼ã¿æ¸ˆã¿ã§ã™"
        }
    
    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    result = await model_manager.load_models()
    return result

@app.get("/api/models/status")
async def get_model_status():
    """ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹å–å¾—"""
    return model_manager.get_model_info()

@app.post("/api/inference/compare")
async def compare_models(request: CompareRequest):
    """ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒæ¨è«–"""
    try:
        # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æº–å‚™
        generation_kwargs = {}
        if request.max_new_tokens:
            generation_kwargs["max_new_tokens"] = request.max_new_tokens
        if request.temperature is not None:
            generation_kwargs["temperature"] = request.temperature
        if request.top_p is not None:
            generation_kwargs["top_p"] = request.top_p
        if request.top_k is not None:
            generation_kwargs["top_k"] = request.top_k
        
        # æ¨è«–å®Ÿè¡Œ
        result = await inference_service.generate_comparison(
            request.prompt, 
            **generation_kwargs
        )
        
        if result["status"] != "success":
            raise HTTPException(status_code=500, detail=result["message"])
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        base_text = result["results"]["base_model"]["text"]
        finetuned_text = result["results"]["finetuned_model"]["text"]
        
        metrics = metrics_calculator.compare_texts(base_text, finetuned_text)
        result["metrics"] = metrics
        
        return result
        
    except Exception as e:
        logger.error(f"æ¯”è¼ƒæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/inference/single")
async def single_inference(request: SingleInferenceRequest):
    """å˜ä¸€ãƒ¢ãƒ‡ãƒ«æ¨è«–"""
    try:
        # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æº–å‚™
        generation_kwargs = {}
        if request.max_new_tokens:
            generation_kwargs["max_new_tokens"] = request.max_new_tokens
        if request.temperature is not None:
            generation_kwargs["temperature"] = request.temperature
        if request.top_p is not None:
            generation_kwargs["top_p"] = request.top_p
        if request.top_k is not None:
            generation_kwargs["top_k"] = request.top_k
        
        # æ¨è«–å®Ÿè¡Œ
        result = await inference_service.generate_single(
            request.prompt,
            request.model_type,
            **generation_kwargs
        )
        
        if result["status"] != "success":
            raise HTTPException(status_code=500, detail=result["message"])
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        text = result["result"]["text"]
        metrics = metrics_calculator.calculate_text_metrics(text)
        result["metrics"] = metrics
        
        return result
        
    except Exception as e:
        logger.error(f"å˜ä¸€æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/settings")
async def get_settings():
    """è¨­å®šæƒ…å ±ã®å–å¾—"""
    return {
        "base_model_name": settings.BASE_MODEL_NAME,
        "finetuned_model_path": settings.FINETUNED_MODEL_PATH,
        "device": settings.DEVICE,
        "max_length": settings.MAX_LENGTH,
        "default_temperature": settings.TEMPERATURE,
        "default_top_p": settings.TOP_P,
        "default_top_k": settings.TOP_K
    }

# ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã‚¤ãƒ™ãƒ³ãƒˆ
@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹æ™‚ã®å‡¦ç†"""
    logger.info("ğŸš€ LLMæ¯”è¼ƒAPI ãŒé–‹å§‹ã•ã‚Œã¾ã—ãŸ")
    logger.info(f"è¨­å®š: {settings.BASE_MODEL_NAME} -> {settings.FINETUNED_MODEL_PATH}")
    logger.info(f"ãƒ‡ãƒã‚¤ã‚¹: {settings.DEVICE}")

@app.on_event("shutdown")
async def shutdown_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã®å‡¦ç†"""
    logger.info("ğŸ›‘ LLMæ¯”è¼ƒAPI ãŒçµ‚äº†ã•ã‚Œã¾ã—ãŸ")

def main():
    """FastAPIã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•"""
    uvicorn.run(
        "webui.api.main:app",
        host=settings.API_HOST,
        port=settings.API_PORT,
        workers=settings.API_WORKERS,
        reload=False,
        log_level=settings.LOG_LEVEL.lower()
    )

if __name__ == "__main__":
    main()
