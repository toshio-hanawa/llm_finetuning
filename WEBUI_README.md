# 🤖 LLM モデル比較 WebUI

**ベースモデル vs ファインチューニング済みモデルの洗練された比較システム**

## ✨ 特徴

### 🎨 モダンで洗練されたデザイン
- **シンプル & エレガント**: ミニマルで直感的なUI
- **レスポンシブデザイン**: あらゆるデバイスで最適表示
- **グラデーション & アニメーション**: 美しい視覚効果
- **ダークモード対応**: 目に優しい表示オプション

### ⚡ 高性能アーキテクチャ
- **FastAPI バックエンド**: 高速で型安全なAPI
- **Streamlit フロントエンド**: リアルタイム更新UI
- **非同期処理**: 並列モデル推論で高速化
- **メモリ効率**: 最適化されたモデル管理

### 📊 包括的な分析機能
- **リアルタイム比較**: 一つのプロンプトで同時推論
- **詳細メトリクス**: 日本語特化の評価指標
- **可視化チャート**: インタラクティブなグラフ
- **履歴管理**: 過去の比較結果を保存

## 🚀 クイックスタート

### 1. 環境セットアップ
```bash
# プロジェクトディレクトリに移動
cd /home/ubuntu/Documents/projects/llm_finetuning

# 仮想環境がない場合は作成
python3 -m venv webui_env
source webui_env/bin/activate

# 依存関係インストール
pip install -r webui_requirements.txt
```

### 2. WebUI起動
```bash
# 統合起動（推奨）
python run_webui.py
```

### 3. アクセス
- **🖥️ Streamlit UI**: http://localhost:8501
- **📚 API ドキュメント**: http://localhost:8000/api/docs
- **💓 ヘルスチェック**: http://localhost:8000/api/health

## 🏗️ アーキテクチャ

```
┌─────────────────────────────────────────────────────────────┐
│                 Streamlit Frontend                          │
│                  (localhost:8501)                          │
├─────────────────────────────────────────────────────────────┤
│                   FastAPI Backend                          │
│                  (localhost:8000)                          │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────────────┐  ┌─────────────────────┐          │
│  │   ベースモデル       │  │ ファインチューニング │          │
│  │ TinyLlama-1.1B-Chat │  │   済みモデル (LoRA)  │          │
│  └─────────────────────┘  └─────────────────────┘          │
└─────────────────────────────────────────────────────────────┘
```

## 🎯 使用方法

### 1. モデル読み込み
1. WebUIにアクセス
2. サイドバーの「📥 モデル読み込み」をクリック
3. 読み込み完了まで待機（初回は数分かかる場合があります）

### 2. 比較実行
1. **プリセット質問**を選択 または **カスタム質問**を入力
2. サイドバーで**推論パラメータ**を調整（オプション）
3. **🚀 比較実行**ボタンをクリック
4. 結果をリアルタイムで確認

### 3. 結果分析
- **📝 モデル出力比較**: 並列表示で違いを確認
- **⚡ パフォーマンス指標**: 速度・文字数の比較
- **📊 詳細メトリクス**: 日本語品質・敬語使用率など
- **📈 可視化分析**: インタラクティブなチャート

## 🔧 手動起動

### FastAPI単体起動
```bash
source webui_env/bin/activate
python -m uvicorn webui.api.main:app --reload --host 0.0.0.0 --port 8000
```

### Streamlit単体起動
```bash
source webui_env/bin/activate
streamlit run webui/app.py --server.port 8501
```

## 📝 API エンドポイント

### ヘルスチェック
```http
GET /api/health
```

### モデル管理
```http
POST /api/models/load       # モデル読み込み
GET  /api/models/status     # モデル状態確認
```

### 推論実行
```http
POST /api/inference/compare  # モデル比較推論
POST /api/inference/single   # 単一モデル推論
```

### 設定取得
```http
GET /api/settings           # システム設定情報
```

## 🎛️ 推論パラメータ

| パラメータ | デフォルト | 説明 |
|-----------|------------|------|
| `max_new_tokens` | 256 | 最大生成トークン数 |
| `temperature` | 0.7 | 生成の創造性（0.1-2.0） |
| `top_p` | 0.9 | 核サンプリング（0.1-1.0） |
| `top_k` | 50 | トップK サンプリング |

## 📊 評価指標

### 基本統計
- 文字数・単語数・文数
- 平均文長・文長分散

### 日本語特性
- ひらがな・カタカナ・漢字使用率
- 文字種多様性

### 言語品質
- 敬語・丁寧語使用率
- 語彙豊富さ
- テキスト類似度

### パフォーマンス
- 推論時間
- メモリ使用量

## 🛠️ トラブルシューティング

### API接続エラー
```bash
# APIサーバーが起動しているか確認
curl http://localhost:8000/api/health

# プロセス確認
ps aux | grep uvicorn
```

### モデル読み込みエラー
```bash
# ファインチューニング済みモデルの存在確認
ls -la japanese_finetuned_model/

# メモリ不足の場合
free -h
```

### パフォーマンス問題
- **CPU使用率が高い場合**: `batch_size`を下げる
- **メモリ不足の場合**: `max_length`を下げる
- **応答が遅い場合**: `max_new_tokens`を下げる

## 🔮 今後の機能拡張

- [ ] **多言語対応**: 英語・中国語など
- [ ] **モデル追加**: より大きなモデルのサポート
- [ ] **RAG機能**: 外部知識ベースとの連携
- [ ] **ユーザー認証**: マルチユーザー対応
- [ ] **結果エクスポート**: PDF・Excel出力
- [ ] **A/Bテスト**: 複数モデルの統計的比較

## 📄 ライセンス

このプロジェクトはMITライセンスの下で公開されています。

## 🤝 コントリビューション

プルリクエストやイシュー報告を歓迎します！

---

**🎉 Happy Comparing! 🤖**
